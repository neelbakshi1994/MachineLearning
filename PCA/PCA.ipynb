{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a technique used to find correlations between different features. Particularly useful when you want to remove a few features which you think do not bring much to the efficieny of your learning algorithm, or if you want to combine two or more features if you think they are representing the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic intuition behind PCA is that, we are trying to measure the data in terms of it's principal components:\n",
    "Prinicipal components are those structures of the data which hold the maximum information about it. In mathematical terms they are the dimensions in which you have the **most variance**. Therefore, in other words it also means that the sum of distance between the points and the principal axis has to be the least.\n",
    "\n",
    "Let's say we have two features, which are plotted with feature-1 on x-axis and feature-2 on y-axis.\n",
    "We can find the principal component axis along which the shadow(mapping) of those data points will yield the maximum variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two quantities w.r.t to PCA which are very important, namely the **eigenvalues** and the **eigenvectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigenvalue** is the number telling you how much variance the data has\n",
    "\n",
    "**Eigenvector** is the direction in which this eigenvalue exists\n",
    "\n",
    "These two always exist in pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us start the theory of our PCA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically do two things:\n",
    "\n",
    "1) Reduce the mean of all our training features ( $j$ training features) to zero\n",
    "$$ \\mu_{j} = \\frac{1}{m} \\sum_{i=1}^{m}x_{j}^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now replace $ x_{j}^{(i)} $ with $x_{j}^{(i)} - \\mu_{j}$ so the the mean of the all the features is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Normalize the feature space\n",
    "$$ \\sigma^{2}_{j} = \\frac{1}{m}\\sum^{m}_{i=1}(x^{(i)}_{j} - \\mu^{(i)})^{2}$$\n",
    "\n",
    "and reaplce each $x^{(i)}_{j}$ with $\\frac{x^{(i)}_{j}}{\\sigma_{j}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
